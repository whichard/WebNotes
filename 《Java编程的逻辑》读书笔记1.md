# Java编程的逻辑

## 编程基础 

### 基本数据类型分类

- 整数类型：有4种整型byte/short/int/long，分别有不同的取值范围: 8, 16, 32, 64位,(1,2,4,8个字节) 存放(-2^7^~2^7^-1, ..., -2^31^~2^31^-1,...)

> 在给long类型赋值时，如果常量超过了int的表示范围，需要在常量后面加大写或小写字母L，因为数字常量默认为是int类型。 类似的, 对于float，需要在数字后面加大写字母F或小写字母f，例如：
> float f=333.33f；这是由于小数常量默认是double类型。

- 小数类型：有两种类型float/double，有不同的取值范围和精度；
- 字符类型：char，表示单个字符；
- 真假类型：boolean，表示真假。

### 数组

数组存两块: 数组内容+数组内容的首地址(类似指针.) 内容空间分配在堆上, 地址指针存放在栈上. 对象与之相似, 有两块内存, 分别存堆栈. 栈空间在出栈时候释放, 堆空间在生命周期结束后GC自动清理.

==号: 对于数组是判断是否指向同一内存地址(数组内容的首地址), 而不是数组内容是否相等(比较数组内容是否一样需要逐个元素对比)

Object的equals方法是调用==.

### switch原理

if、if/else、if/else if/else、三元运算符都会转换为条件跳转和无条件跳转，但switch不太一样。
switch的转换和具体系统实现有关。如果分支比较少，可能会转换为跳转指令。如果分支比较多，使用条件跳转会进行很多次的比较运算，效率比较低，可能会使用一种更为高效的方式，叫**跳转表**。跳转表是一个映射表，存储了可能的值以及要跳转到的地址。

跳转表为什么会更为高效呢？因为其中的值必须为整数，且按大小顺序排序。按大小排序的整数可以使用高效的二分查找，即先与中间的值比，如果小于中间的值，则在开始和中间值之间找，否则在中间值和末尾值之间找，每找一次缩小一半查找范围。如果值是连续的，则跳转表还会进行特殊优化，优化为一个数组，连找都不用找了，值就是数组的下标索引，直接根据值就可以找到跳转的地址。即使值不是连续的，但数字比较密集，差的不多，编译器也可能会优化为一个数组型的跳转表，没有的值指向default分支。

程序源代码中的case值排列不要求是排序的，编译器会自动排序。之前说switch值的类型可以是byte、short、int、char、枚举和String。其中byte/short/int本来就是整数，char本质上也是整数（2.4节介绍），而枚举类型也有对应的整数（5.4节介绍），String用于switch时也会转换为整数。**不可以使用long，为什么呢？跳转表值的存储空间一般为32位，容纳不下long。**简单说明下String，**String是通过hashCode方法（7.2节介绍）转换为整数的**，但不同String的hashCode可能相同，跳转后会再次根据String的内容进行比较判断。


### 函数调用

我们之前谈过程序执行的基本原理：CPU有一个指令指示器，指向下一条要执行的指令，要么顺序执行，要么进行跳转（条件跳转或无条件跳转）。
基本上，这依然是成立的，程序从main函数开始顺序执行，函数调用可以看作一个无条件跳转，跳转到对应函数的指令处开始执行，碰到return语句或者函数结尾的时候，再执行一次无条件跳转，跳转回调用方，执行调用函数后的下一条指令。

计算机系统主要**使用栈来存放函数调用过程中需要的数据**，包括参数、返回地址，以及函数内定义的局部变量。计算机系统就如何在栈中存放这些数据，调用者和函数如何协作做了约定。返回值不太一样，它可能放在栈中，但它使用的栈和局部变量不完全一样，有的系统使用CPU内的一个存储器存储返回值，我们可以简单认为**存在一个专门的返回值存储器**。main函数的相关数据放在栈的最下面，每调用一次函数，都会将相关函数的数据入栈，调用结束会出栈。

对于数组arr(对象也是)，在栈中存放的是实际内容的地址0x1000，存放地址的栈空间会随着入栈分配，出栈释放，但存放实际内容的堆空间不受影响。(由GC自动回收)

#### 递归

以上就是递归函数的执行过程，函数代码虽然只有一份，但在执行的过程中，每调用一次，就会有一次入栈，生成一份不同的参数、局部变量和返回地址。(直到遇到终结条件才出现返回值. 在此之前返回值存储器都是空的)

## 进制

整数类型：有4种整型byte/short/int/long，分别有不同的取值范围: 8, 16, 32, 64位,(1,2,4,8个字节) 存放(-2^7^~2^7^-1, ..., -2^31^~2^31^-1,...)

### 位权

一个数字的每个位置都存在一个位权

 123(10进制)表示1×（10^2^）+2×（10^1^）+3×（10^0^）

1110(2进制)=1x(2^3^)+1x(2^2^)+1x(2^1^)+0x(2^0^)

`0.1f != 0.1f*0.1f`: 二进制的小数位权为2^-1^, 2^-2^…,只能精确表示这些数字的组合.

### 负数的二进制表示

十进制的负数表示就是在前面加一个负数符号-，例如-123。但二进制如何表示负数呢？其实概念是类似的，二进制使用最高位表示符号位，用1表示负数，用0表示正数。但哪个是最高位呢？整数有4种类型byte、short、int、long，分别占1、2、4、8个字节，即分别占8、16、32、64位，每种类型的符号位都是其最左边的一位。为方便举例，下面假定类型是byte，即从右到左的第8位表示符号位。

#### 补码表示法

负数的二进制表示就是**对应的正数的补码**表示.

补码表示就是在原码表示的基础上**按位取反然后加1**。取反就是将0变为1，1变为0。

1. -1：1的原码表示是00000001，取反是11111110，然后再加1，就是11111111。
2. -2：2的原码表示是00000010，取反是11111101，然后再加1，就是11111110。
3. -127：127的原码表示是01111111，取反是10000000，然后再加1，就是10000001。
4. 127+1=-128: 01111111+1=10000000, 就是-128

##### 逆运算&为什么用补码

给定一个负数的二进制表示，要想知道它的十进制值，可以采用相同的补码运算。比如：
10010010，首先取反，变为01101101，然后加1，结果为01101110，它的十进制值为110，所以原值就是-110。直觉上，应该是先减1，然后再取反，但**计算机只能做加法**，**而补码的一个良好特性就是，对负数的补码表示做补码运算就可以得到其对应正数的原码**，**正如十进制运算中负负得正一样(取反再加一, 逆运算也是如此, 这一特性很方便)**。

负整数为什么要采用这种奇怪的表示形式呢？原因是，**只有这种形式，计算机才能实现正确的减法和负数加法**.

计算结果超出表示范围,  会被认为表示最大负数. byte(-2^7^), short(-2^15^)

### 位运算

左移<< 右边低位补0, 左边高位**舍弃,** 二进制: 移动一位相当于x2

**无符号**右移>>>左边高位**补0**, 右边**低位舍弃**

有符号右移：操作符为>>，向右移动，右边的舍弃掉，左边补什么取决于原来最高位是什么，原来是1就补1，原来是0就补0，将二进制看作整数，右移1位相当于除以2。注意：负数的最高位1也会右移，所以所有的负数在右移多次之后都会变成-1(每位都是1)

### 逻辑运算

计算机进行逻辑运算和位运算效率远远高于其他运算(如+-), 所以底层的 **高效率**的实现往往是使用一些比较复杂的位运算+逻辑运算来实现简单的操作.<Hacker’s Delight>一书多有记载.

逻辑运算有以下几种。
按位与&：两位都为1才为1。
按位或l：只要有一位为1，就为1。
按位取反~：1变为0，0变为1。
按位异或：相异为真，相同为假。
大部分都比较简单，如下所示，具体就不赘述了。

```java
int a=…；
a=a&0x1//返回0或1，就是a最右边一位的值
a=a&0x1111//取a的最低4位, 高位舍弃
a=a |0x1//不管a原来最右边一位是什么，都将设为1
```

### 小数

12.345 = 1×10+2×1+3×0.1+4×0.01+5×0.001，与整数的表示类似，小数点后面的每个位置也都有一个位权，从左到右，依次为0.1，0.01，0.001...即10^（-1）^，10^（-2）^，10^（-3）^等。

很多数十进制也是不能精确表示的，比如1/3. **十进制也只能表示那些可以表述为10的多少次方和的数.二进制是类似的，但二进制只能表示那些可以表述为2的多少次方和的数。**

2^-1^=0.5, 2^-2^=0.25, 2^-3^ = 0.125 

如果编写程序进行试验，会发现有的计算结果是准确的。比如，用Java写

```java
System.out.println（0.1f+0.1f）；
System.out.print1n（0.1f*0.1f）；
```

第一行输出0.2，第二行输出0.010000001。按照上面的说法，第一行的结果应该也不对。其实，这只是Java语言给我们造成的假象，计算结果其实也是不精确的，但是由于结果和0.2足够接近，在输出的时候，Java选择了输出0.2这个看上去非常精简的数字，而不是一个中间有很多0的小数。在误差足够小的时候，结果看上去是精确的，但**不精确其实才是常态**。

计算不精确，怎么办呢？

- 大部分情况下，我们不需要那么高的精度，可以四舍五入，或者在输出的时候只保留固定个数的小数位(直接舍弃尾数)。
- 如果真的需要比较高的精度，一种方法是将小数转化为整数进行运算，运算结束后再转化为小数(左移然后右移)
- 另一种方法是使用十进制的数据类型，这个并没有统一的规范。在Java中是BigDecimal

### 编码乱码

ASCII: American standard code for internet interchange

ASCII码是基础，使用一个字节表示，**最高位设为0，其他7位表示128个字符**(巧妙设计, 让其他字符码都可以兼容ASCII)。其他编码都是兼容ASCII的，最高位使用1来进行区分。
西欧主要使用Windows-1252，使用一个字节，增加了额外128个字符。
我国内地的三个主要编码GB2312、GBK、GB18030有时间先后关系，表示的字符数越来越多，且后面的兼容前面的，GB2312和GBK都是用两个字节表示，而GB18030则使用两个或四个字节表示。
我国香港特别行政区和我国台湾地区的主要编码是Big5。
如果文本里的字符都是ASCI码字符，那么采用以上所说的任一编码方式都是一样的。
但如果有高位为1的字符，除了GB2312、GBK、GB18030外，其他编码都是不兼容的。比如，Windows-1252和中文的各种编码是不兼容的，即使Big5和GB18030都能表示繁体字，其表示方式也是不一样的，而这就会出现所谓的乱码，具体我们稍后介绍。

Unicode给世界上所有字符都规定了一个统一的编号，编号范围达到110多万，但大部分字符都在65536以内。Unicode本身没有规定怎么把这个编号对应到二进制形式。
UTF-32/UTF-16/UTF-8都在做一件事，就是把Unicode编号对应到二进制形式，其对应方法不同而已。UTF-32使用4个字节，UTF-16大部分是两个字节，少部分是4个字节，它们都不兼容ASCⅡ编码，都有字节顺序的问题。UTF-8使用1~4个字节表示，兼容ASCI编码，英文字符使用1个字节，中文字符大多用3个字节。

BE: BigEndien LE: LittleEndien 

> **大端模式：**
>
> 低地址 -----------------> 高地址
> 0x12  |  0x34  |  0x56  |  0x78
>
> **2)小端模式：**
>
> 低地址 ------------------> 高地址
> 0x78  |  0x56  |  0x34  |  0x12

记忆：big-endien，end with big， 顺序存储。 


UTF-16BE: 大端的UTF-16

#### 编码转换

编码转换的具体过程可以是：一个字符从A编码转到B编码，先找到字符的A编码格式，通过A的映射表找到其Unicode编号，然后通过Unicode编号再查B的映射表，找到字符的B编码格式。
举例来说，“马”从GB18030转到UTF-8，先查GB18030>Unicode编号表，得到其编号是9A6C，然后查Uncode编号>UTF-8表，得到其UTF-8编码：E9A9AC。
编码转换改变了字符的二进制内容，但并没有改变字符看上去的样子。

#### 乱码

理解了编码，我们来看乱码。乱码有两种常见原因：一种比较简单，就是简单的解析错误；另外一种比较复杂，在错误解析的基础上进行了编码转换。我们分别介绍。

##### 解析错误

这种情况下，之所以看起来是乱码，是因为看待或者说解析数据的方式错了。只要使用正确的编码方式进行解读就可以纠正了。很多文件编辑器，如EditPlus、NotePad++、UltraEdit都有切换查看编码方式的功能，浏览器也都有切换查看编码方式的功能，如Fire-fox，在菜单“查看”一“文字编码”中即可找到该功能。
切换查看编码的方式并没有改变数据的二进制本身，而只是改变了解析数据的方式，从而改变了数据看起来的样子，这与前面提到的编码转换正好相反。很多时候，做这样一个编码查看方式的切换就可以解决乱码的问题，但有的时候这样是不够的。

##### 错误的解析和编码转换

文本在错误解析的基础上还进行了编码转换. 例子: 

1）两个字“老马”，本来的编码格式是GB18030，编码（十六进制）是COCF C2ED。
2）这个二进制形式被错误当成了Windows-1252编码，解读成了字符“AlAr'。
3）随后这个字符进行了编码转换，转换成了UTF-8编码，形式还是“AYA”，但二进制变成了C380C38FC382C3AD，每个字符两个字节。
4）这个时候再按照GB18030解析，字符就变成了乱码形式“胶胳柳”，而且这时无论怎么切换查看编码的方式，这个二进制看起来都是乱码。
这种情况是乱码产生的主要原因。

这种情况其实很常见，计算机程序为了便于统一处理，经常会将所有编码转换为一种方式，比如UTF-8，在转换的时候，需要知道原来的编码是什么，但可能会搞错，而一旦搞错并进行了转换，就会出现这种乱码。这种情况下，无论怎么切换查看编码方式都是不行的。

#### 乱码恢复

“乱”主要是因为发生了一次错误的编码转换，所谓恢复，是指要恢复两个关键信息：一个是原来的二进制编码方式A；另一个是错误解读的编码方式B。
恢复的基本思路是尝试进行逆向操作，假定按一种编码转换方式B获取乱码的二进制格式，然后再假定一种编码解读方式A解读这个二进制，查看其看上去的形式，这要尝试多种编码，如果能找到看着正常的字符形式，应该就可以恢复。

详细实例见Book. 实际中，我们可以写一个循环，测试不同的A/B编码中的结果形式，Java编程实现: 

```java
public static void recover（String str）
	throws UnsupportedEncodingException{
String[]charsets=new String[]{
	"windows-1252"，"GB18030"，"Big5"，"UTF-8"}；
for（int i=0；i<charsets.1ength；i++）{
	for（int j=0；j<charsets.1ength；j++）{
		if（i!=j）{
		String s=new String（str.getBytes（charsets[i]），charsets[j]）；			 	
		System.out.println（"----原来编码（A）假设是：“
			+charsets[j]+"，被错误解读为了（B）："+charsets[i]）；
System.out.println（s）；
System.out.println（）；
}
```

以上代码使用不同的编码格式进行测试，如果输出有正确的，那么就可以恢复。

不是所有的乱码形式都是可以恢复的，如果形式中有很多不能识别的字符（如？），则很难恢复。
另外，**如果乱码是由于进行了多次解析和转换错误造成的，也很难恢复**。 比如以上的代码就是处理单次乱码转换的, 多次乱码转换可能性太多, 而且容易出现不能恢复的字符.

### char

在**Java内部进行字符处理时，采用的都是Unicode，具体编码格式是UTF-16BE**(Big Endien)。简单回顾一下，UTF-16使用两个或4个字节表示一个字符，Unicode编号范围在65536以内的占两个字节，超出范围的占4个字节，BE就是先输出高位字节，再输出低位字节，这与整数的内存表示是一致的。
char**本质上是一个固定占用两个字节的无符号正整数**，**这个正整数对应于Unicode编号**，用于表示那个Unicode编号对应的字符。由于固定占用两个字节，**char只能表示Unicode编号在65536以内的字符，而不能表示超出范围的字符。那超出范围的字符怎么表示呢？使用两个char。**类Character、String有一些相关的方法，我们到第7章再介绍。

**char的加减运算就是按其Unicode编号进行运算**，一般对字符做加减运算没什么意义，但ASCII码字符是有意义的。比如大小写转换，大写A-Z的编号是65-90，小写a-z的编号是97-122，正好相差32，所以大写转小写只需加32，而小写转大写只需减32。加减运算的另一个应用是加密和解密，将字符进行某种可逆的数学运算可以做加解密。
char的位运算可以看作是对应整数的位运算，只是**它是无符号数**，也就是说，**有符号右移>>和无符号右移>>>的结果是一样的**。既然char本质上是整数，查看char的二进制表示，同样可以用Integer的方法
